{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2Model\n",
    "\n",
    "import librosa\n",
    "from librosa.effects import trim\n",
    "import librosa.display\n",
    "\n",
    "import soundfile\n",
    "\n",
    "import scipy as sy\n",
    "import pydub as pyd\n",
    "\n",
    "import jiwer as jw\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounting Gdrive for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still finding alternatives to access dataset on gdrive without downloading the datasets locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset/\"\n",
    "metatadata_path = \"metadata\"\n",
    "\n",
    "audio_directory = \"dataset/\"\n",
    "\n",
    "audio_data = []\n",
    "\n",
    "# Create a dataframe for the metadata\n",
    "# dataframe = pd.read_csv(metatadata_path)\n",
    "\n",
    "# Display dataframe to check if ready\n",
    "\n",
    "# print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: dataset/01F_1.wav, Signal Length: [-0.01692389 -0.02734679 -0.02197435 ... -0.00276161 -0.00074782\n",
      " -0.00303257]\n"
     ]
    }
   ],
   "source": [
    "# Load the audio files and preprocess\n",
    "\n",
    "sampling_rate = 16000\n",
    "\n",
    "for filename in os.listdir(audio_directory):\n",
    "    if(filename.endswith(\".wav\")):\n",
    "        file_path = os.path.join(audio_directory, filename)\n",
    "        \n",
    "        # Load file with Sampling Rate of 16000 Hz\n",
    "        signal, sr = librosa.load(file_path, sr=sampling_rate)\n",
    "        \n",
    "        # Normalizing the audio data\n",
    "        signal = signal / max(abs(signal))\n",
    "        \n",
    "        # Trim silence\n",
    "        signal, _ = trim(signal)\n",
    "        \n",
    "        # Appending the data to a variable\n",
    "        audio_data.append({\"file_path\": file_path, \"signal\": signal, \"Sampling Rate\": sr })\n",
    "\n",
    "\n",
    "# Check if audio_data is ready\n",
    "\n",
    "print(f\"File: {audio_data[0]['file_path']}, Signal Length: {audio_data[0]['signal']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Mel Spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'figure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m mel_spectrogram_db \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39mpower_to_db(mel_spectrogram, ref\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mmax)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Plot the Mel spectrogram\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m     11\u001b[0m librosa\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mspecshow(mel_spectrogram_db, sr\u001b[38;5;241m=\u001b[39msr, x_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m, y_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel\u001b[39m\u001b[38;5;124m'\u001b[39m, fmax\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8000\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar(\u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%+2.0f\u001b[39;00m\u001b[38;5;124m dB\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Add a color bar for amplitude\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marwin Jay\\Desktop\\ASR Client\\asr-thesis-2024\\asr_env\\Lib\\site-packages\\matplotlib\\_api\\__init__.py:217\u001b[0m, in \u001b[0;36mcaching_module_getattr.<locals>.__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m props:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m props[name]\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'figure'"
     ]
    }
   ],
   "source": [
    "test_audio = \"01F_1.wav\"\n",
    "sample_audio = os.path.join(dataset_path, test_audio)\n",
    "\n",
    "y, sr = librosa.load(sample_audio, sr=sampling_rate)\n",
    "\n",
    "mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000)\n",
    "mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "\n",
    "# Plot the Mel spectrogram\n",
    "plt.figure(figsize=(10, 4))\n",
    "librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel', fmax=8000, cmap='viridis')\n",
    "plt.colorbar(format='%+2.0f dB')  # Add a color bar for amplitude\n",
    "plt.title('Mel Spectrogram')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Mel Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Wav2Vec model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Define a custom model\n",
    "class ASRModel(tf.keras.Model):\n",
    "    def __init__(self, wav2vec_model, vocab_size):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.wav2vec = wav2vec_model\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        wav2vec_outputs = self.wav2vec(inputs).last_hidden_state\n",
    "        logits = self.dense(wav2vec_outputs)\n",
    "        return logits\n",
    "\n",
    "# Define the vocabulary size (number of tokens)\n",
    "vocab_size = len(processor.tokenizer)\n",
    "model = ASRModel(wav2vec_model, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess_audio(file_path):\n",
    "    audio, _ = librosa.load(file_path, sr=16000)\n",
    "    return audio\n",
    "\n",
    "# Function to tokenize transcriptions\n",
    "def tokenize_transcription(transcription):\n",
    "    return processor.tokenizer(transcription).input_ids\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "def prepare_dataset(file_paths, transcriptions):\n",
    "    audio_features = [preprocess_audio(fp) for fp in file_paths]\n",
    "    labels = [tokenize_transcription(t) for t in transcriptions]\n",
    "    return tf.data.Dataset.from_tensor_slices((audio_features, labels))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Results using MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
