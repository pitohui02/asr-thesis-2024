{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all packages here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\asr-thesis-2024\\asr_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marwin Jay\\Desktop\\ASR Client\\asr-thesis-2024\\asr_env\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import Wav2Vec2Processor, TFWav2Vec2Model\n",
    "\n",
    "import librosa\n",
    "from librosa.effects import trim\n",
    "\n",
    "import soundfile\n",
    "\n",
    "import scipy as sy\n",
    "import pydub as pyd\n",
    "\n",
    "import jiwer as jw\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mounting Gdrive for the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still finding alternatives to access dataset on gdrive without downloading the datasets locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset/\"\n",
    "metatadata_path = \"metadata\"\n",
    "\n",
    "audio_directory = \"dir\"\n",
    "\n",
    "audio_data = []\n",
    "\n",
    "# Create a dataframe for the metadata\n",
    "dataframe = pd.read_csv(metatadata_path)\n",
    "\n",
    "# Display dataframe to check if ready\n",
    "\n",
    "print(dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio files and preprocess\n",
    "\n",
    "for filename in os.listdir(audio_directory):\n",
    "    if(filename.endswith(\".wav\")):\n",
    "        file_path = os.path.join(audio_directory, filename)\n",
    "        \n",
    "        # Load file with Sampling Rate of 16000 Hz\n",
    "        signal, sr = librosa.load(file_path, sr=16000)\n",
    "        \n",
    "        # Normalizing the audio data\n",
    "        signal = signal / max(abs(signal))\n",
    "        \n",
    "        # Trim silence\n",
    "        signal, _ = trim(signal)\n",
    "        \n",
    "        # Appending the data to a variable\n",
    "        audio_data.append({\"file_path\": file_path, \"signal\": signal, \"Sampling Rate\": sr })\n",
    "\n",
    "\n",
    "# Check if audio_data is ready\n",
    "\n",
    "print(f\"File: {audio_data[0]['file_path']}, Signal Length: {audio_data[0]['signal']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained Wav2Vec model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "wav2vec_model = TFWav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Define a custom model\n",
    "class ASRModel(tf.keras.Model):\n",
    "    def __init__(self, wav2vec_model, vocab_size):\n",
    "        super(ASRModel, self).__init__()\n",
    "        self.wav2vec = wav2vec_model\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        wav2vec_outputs = self.wav2vec(inputs).last_hidden_state\n",
    "        logits = self.dense(wav2vec_outputs)\n",
    "        return logits\n",
    "\n",
    "# Define the vocabulary size (number of tokens)\n",
    "vocab_size = len(processor.tokenizer)\n",
    "model = ASRModel(wav2vec_model, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess audio\n",
    "def preprocess_audio(file_path):\n",
    "    audio, _ = librosa.load(file_path, sr=16000)\n",
    "    return audio\n",
    "\n",
    "# Function to tokenize transcriptions\n",
    "def tokenize_transcription(transcription):\n",
    "    return processor.tokenizer(transcription).input_ids\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "def prepare_dataset(file_paths, transcriptions):\n",
    "    audio_features = [preprocess_audio(fp) for fp in file_paths]\n",
    "    labels = [tokenize_transcription(t) for t in transcriptions]\n",
    "    return tf.data.Dataset.from_tensor_slices((audio_features, labels))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of Results using MatPlotLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "asr_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
